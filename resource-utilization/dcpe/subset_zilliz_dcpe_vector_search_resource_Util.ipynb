{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codeby3/searchable-encryption/blob/resource-utilization/resource-utilization/dcpe/subset_zilliz_dcpe_vector_search_resource_Util.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd26fRC07BGs"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ele1IiqmR5O7"
      },
      "outputs": [],
      "source": [
        "!pip install -qU beir sentence-transformers pymilvus datasets pycryptodome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsT0EcFZBh_w"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUKH5xL6yQY-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from beir import util\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "from beir.retrieval.evaluation import EvaluateRetrieval\n",
        "import pandas as pd\n",
        "from pymilvus import MilvusClient, FieldSchema, DataType, CollectionSchema, Collection\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import base64\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaH6m8MoUV4G"
      },
      "outputs": [],
      "source": [
        "sys.path.append(os.getcwd())\n",
        "import dcpe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjS56drAW9Et"
      },
      "outputs": [],
      "source": [
        "DCPE_KEY = dcpe.DCPEKey.generate_random(scaling_factor=1.2) # Using a scaling factor of 1.2\n",
        "DCPE_APPROXIMATION_FACTOR = 1.0 # Using an approximation factor of 1.0 for a good tradeoff between sceurity and accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgaeW_U57GEJ"
      },
      "source": [
        "# Load Datasets\n",
        "We're using 3 datasets from the BEIR datasets - nfcorpus, fiqa and scidocs (https://huggingface.co/datasets/BeIR/beir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nzKsJgx31vU"
      },
      "outputs": [],
      "source": [
        "datasets_to_load = [\"nfcorpus\", \"fiqa\", \"scidocs\"]\n",
        "beir_data_path = \"./beir_datasets\" # Local directory to store BEIR data\n",
        "os.makedirs(beir_data_path, exist_ok=True)\n",
        "loaded_beir_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJJq83bw4NJ3"
      },
      "outputs": [],
      "source": [
        "for dataset_name in datasets_to_load:\n",
        "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
        "\n",
        "    # Step 3a: Download the dataset\n",
        "    url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip\"\n",
        "    out_dir = os.path.join(beir_data_path, dataset_name)\n",
        "\n",
        "    if not os.path.exists(out_dir):\n",
        "        print(f\"Downloading {dataset_name} from {url} to {out_dir}...\")\n",
        "        data_path = util.download_and_unzip(url, out_dir)\n",
        "        print(f\"Downloaded {dataset_name} to: {data_path}\")\n",
        "    else:\n",
        "        print(f\"Dataset {dataset_name} already exists at {out_dir}. Skipping download.\")\n",
        "        data_path = out_dir\n",
        "\n",
        "    # Step 3b: Load the corpus, queries, and qrels\n",
        "    try:\n",
        "        corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
        "        loaded_beir_data[dataset_name] = {\n",
        "            \"corpus\": corpus,\n",
        "            \"queries\": queries,\n",
        "            \"qrels\": qrels\n",
        "        }\n",
        "        print(f\"Successfully loaded data for {dataset_name}.\")\n",
        "        print(f\"  Corpus size: {len(corpus)} documents\")\n",
        "        print(f\"  Queries size: {len(queries)} queries\")\n",
        "        print(f\"  Qrels size: {len(qrels)} relevance judgments\")\n",
        "\n",
        "        # Print a sample document and query to verify\n",
        "        if len(corpus) > 0:\n",
        "            sample_doc_id = list(corpus.keys())[0]\n",
        "            print(f\"  Sample corpus entry ({sample_doc_id}): {corpus[sample_doc_id]['title']} - {corpus[sample_doc_id]['text'][:100]}...\")\n",
        "        if len(queries) > 0:\n",
        "            sample_query_id = list(queries.keys())[0]\n",
        "            print(f\"  Sample query entry ({sample_query_id}): {queries[sample_query_id][:100]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvbxK7o_gnOm"
      },
      "source": [
        "### Using subsets of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXkMNge-goVz"
      },
      "outputs": [],
      "source": [
        "SUBSET_QUERY_PERCENTAGE = 0.05\n",
        "RANDOM_SEED = 42 # VERY IMPORTANT: Use a fixed seed for reproducibility!\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "subset_beir_data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCR6_Iy8gqDV"
      },
      "outputs": [],
      "source": [
        "for dataset_name, data in loaded_beir_data.items():\n",
        "    print(f\"\\nCreating subset for dataset: {dataset_name}\")\n",
        "\n",
        "    corpus = data[\"corpus\"]\n",
        "    queries = data[\"queries\"]\n",
        "    qrels = data[\"qrels\"]\n",
        "\n",
        "    # Create a subset of queries ---\n",
        "    num_queries_subset = int(len(queries) * SUBSET_QUERY_PERCENTAGE)\n",
        "    query_ids = list(queries.keys())\n",
        "    # Ensure there are queries to sample from\n",
        "    if num_queries_subset > 0 and len(query_ids) >= num_queries_subset:\n",
        "        subset_query_ids = set(random.sample(query_ids, num_queries_subset))\n",
        "    else:\n",
        "        subset_query_ids = set(query_ids) # Use all if subset is too small or percentage is 100%\n",
        "\n",
        "    subset_queries = {qid: queries[qid] for qid in subset_query_ids}\n",
        "    print(f\"  Selected {len(subset_queries)} queries (out of {len(queries)})\")\n",
        "\n",
        "    # Create a corpus subset containing only relevant documents for the query subset ---\n",
        "    # This is a better approach than random corpus sampling for ensuring evaluation is meaningful.\n",
        "    relevant_corpus_ids = set()\n",
        "    for qid in subset_query_ids:\n",
        "        if qid in qrels:\n",
        "            for doc_id, score in qrels[qid].items():\n",
        "                if score > 0: # A score > 0 indicates relevance\n",
        "                    relevant_corpus_ids.add(doc_id)\n",
        "\n",
        "    # Filter the main corpus to only include these relevant documents\n",
        "    subset_corpus = {cid: corpus[cid] for cid in relevant_corpus_ids if cid in corpus}\n",
        "    print(f\"  Selected {len(subset_corpus)} documents relevant to the query subset.\")\n",
        "\n",
        "    # Filter qrels to match the new query and corpus subsets ---\n",
        "    subset_qrels = {}\n",
        "    for qid, doc_scores in qrels.items():\n",
        "        if qid in subset_query_ids: # Only consider queries in our subset\n",
        "            filtered_scores = {did: score for did, score in doc_scores.items() if did in subset_corpus}\n",
        "            if filtered_scores: # Only add the query if it has relevant docs in the subset\n",
        "                subset_qrels[qid] = filtered_scores\n",
        "    print(f\"  Filtered qrels to {len(subset_qrels)} queries with relevant documents in the new corpus subset.\")\n",
        "\n",
        "\n",
        "    # Store the new subset data ---\n",
        "    subset_beir_data[dataset_name] = {\n",
        "        \"corpus\": subset_corpus,\n",
        "        \"queries\": subset_queries,\n",
        "        \"qrels\": subset_qrels\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWIoxEHH7S4J"
      },
      "source": [
        "# Ingest data in Zilliz Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsoo7s3L4RNY"
      },
      "outputs": [],
      "source": [
        "ZILLIZ_CLOUD_URI = userdata.get(\"ZILLIZ_ENDPOINT\")\n",
        "ZILLIZ_CLOUD_API_KEY = userdata.get(\"ZILLIZ_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v19uRfE3ENYK"
      },
      "outputs": [],
      "source": [
        "zilliz_client = MilvusClient(\n",
        "        uri=ZILLIZ_CLOUD_URI,\n",
        "        token=ZILLIZ_CLOUD_API_KEY\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kKr-cGF7ef5"
      },
      "source": [
        "Using all-MiniLM-L6-v2 embedding model from HuggingFace SentenceTransformers Library. It generates embeddings of 384 dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vD4EAxbEW01"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load the model\n",
        "try:\n",
        "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
        "    print(f\"Model 'all-MiniLM-L6-v2' loaded successfully on {device}.\")\n",
        "    # Verify model output dimension (all-MiniLM-L6-v2 has 384 dimensions)\n",
        "    dummy_embedding = embedding_model.encode(\"test sentence\")\n",
        "    print(f\"Model output dimension: {len(dummy_embedding)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load embedding model: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDb7ObBl7wFX"
      },
      "source": [
        "Batch ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUheTEgNFARP"
      },
      "outputs": [],
      "source": [
        "DIMENSION = 384 # Dimension for all-MiniLM-L6-v2\n",
        "BATCH_SIZE = 64\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "ingestion_metrics = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUW7qqZkG9tF"
      },
      "outputs": [],
      "source": [
        "for dataset_name in subset_beir_data:\n",
        "    print(f\"\\nProcessing Zilliz ingestion for dataset: {dataset_name}\")\n",
        "    collection_name = f\"subset_beir_{dataset_name.replace('-', '_')}_dcpe\"\n",
        "\n",
        "    if zilliz_client.has_collection(collection_name=collection_name):\n",
        "        print(f\"Collection '{collection_name}' already exists. Dropping and recreating.\")\n",
        "        zilliz_client.drop_collection(collection_name=collection_name)\n",
        "\n",
        "    # Define fields\n",
        "    fields = [\n",
        "        FieldSchema(name=\"doc_id\", dtype=DataType.VARCHAR, max_length=512, is_primary=True),\n",
        "        FieldSchema(name=\"title\", dtype=DataType.VARCHAR, max_length=512),\n",
        "        FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
        "        FieldSchema(name=\"encrypted_vector\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION),\n",
        "        FieldSchema(name=\"iv\", dtype=DataType.VARCHAR, max_length=64),\n",
        "        FieldSchema(name=\"auth_hash\", dtype=DataType.VARCHAR, max_length=64)\n",
        "    ]\n",
        "    schema = CollectionSchema(fields, description=f\"BEIR {dataset_name} DCPE encrypted documents\")\n",
        "\n",
        "    print(f\"Creating collection '{collection_name}'...\")\n",
        "    zilliz_client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        schema=schema,\n",
        "        consistency_level=\"Strong\"\n",
        "    )\n",
        "    print(f\"Collection '{collection_name}' created successfully.\")\n",
        "\n",
        "    corpus = subset_beir_data[dataset_name][\"corpus\"]\n",
        "    doc_ids = list(corpus.keys())\n",
        "    num_documents = len(doc_ids)\n",
        "\n",
        "    start_total_ingestion = time.perf_counter()\n",
        "\n",
        "    documents_to_embed = []\n",
        "    original_titles = []\n",
        "    original_texts = []\n",
        "\n",
        "    for doc_id in doc_ids:\n",
        "        title = corpus[doc_id].get(\"title\", \"\")\n",
        "        text = corpus[doc_id].get(\"text\", \"\")\n",
        "        content_to_embed = f\"{title} {text}\".strip()\n",
        "        documents_to_embed.append(content_to_embed)\n",
        "        original_titles.append(title)\n",
        "        original_texts.append(text)\n",
        "\n",
        "    print(f\"Generating embeddings and inserting encrypted data for '{dataset_name}' corpus ({len(doc_ids)} documents)...\")\n",
        "\n",
        "    # Lists to hold encrypted components for batch insertion\n",
        "    batch_encrypted_vectors_list = []\n",
        "    batch_ivs_list = []\n",
        "    batch_auth_hashes_list = []\n",
        "    batch_doc_ids_list = []\n",
        "    batch_titles_list = []\n",
        "    batch_texts_list = []\n",
        "\n",
        "    for i in tqdm(range(0, len(doc_ids), BATCH_SIZE), desc=f\"Ingesting {dataset_name}\"):\n",
        "        batch_current_doc_ids = doc_ids[i:i + BATCH_SIZE]\n",
        "        batch_contents = documents_to_embed[i:i + BATCH_SIZE]\n",
        "        batch_titles = original_titles[i:i + BATCH_SIZE]\n",
        "        batch_texts = original_texts[i:i + BATCH_SIZE]\n",
        "\n",
        "        # Generate original embeddings\n",
        "        original_batch_vectors = embedding_model.encode(batch_contents, convert_to_list=True)\n",
        "\n",
        "        # Encrypt each vector in the batch\n",
        "        encrypted_batch_vectors = []\n",
        "        encrypted_batch_ivs = []\n",
        "        encrypted_batch_auth_hashes = []\n",
        "\n",
        "        for vec in original_batch_vectors:\n",
        "           encrypted_result = dcpe.encrypt_vector(DCPE_KEY, vec.tolist(), DCPE_APPROXIMATION_FACTOR)\n",
        "           encrypted_batch_vectors.append(encrypted_result.ciphertext)\n",
        "           encrypted_batch_ivs.append(base64.b64encode(encrypted_result.iv).decode('utf-8'))\n",
        "           encrypted_batch_auth_hashes.append(base64.b64encode(encrypted_result.auth_hash.get_bytes()).decode('utf-8'))\n",
        "\n",
        "        # Prepare entities for insertion\n",
        "        entities = []\n",
        "        for j in range(len(batch_current_doc_ids)):\n",
        "            entities.append({\n",
        "                \"doc_id\": batch_current_doc_ids[j],\n",
        "                \"title\": batch_titles[j],\n",
        "                \"text\": batch_texts[j],\n",
        "                \"encrypted_vector\": encrypted_batch_vectors[j], # Use encrypted vector\n",
        "                \"iv\": encrypted_batch_ivs[j],\n",
        "                \"auth_hash\": encrypted_batch_auth_hashes[j]\n",
        "            })\n",
        "\n",
        "        zilliz_client.insert(collection_name=collection_name, data=entities)\n",
        "\n",
        "    end_total_ingestion = time.perf_counter()\n",
        "    total_ingestion_time = end_total_ingestion - start_total_ingestion\n",
        "    print(f\"Finished inserting encrypted data for '{dataset_name}'. Total ingestion time: {total_ingestion_time:.2f} seconds\")\n",
        "\n",
        "    # Estimated size calculation to account for VARCHAR storage\n",
        "    # Encrypted vector: DIMENSION * 4 bytes (float32)\n",
        "    # IV (Base64): 12 bytes raw -> ~16-24 chars, let's assume average 20 bytes/char for VARCHAR storage if not compressed.\n",
        "    # Auth Hash (Base64): 32 bytes raw -> ~44-48 chars, assume average 46 bytes/char.\n",
        "    # A VARCHAR(64) field will consume more than just raw byte size, depending on encoding.\n",
        "    # For simplicity in estimation, we'll use the max_length for VARCHAR fields.\n",
        "    estimated_single_record_size_bytes = (DIMENSION * 4) + (64) + (64) # Vector + max_length for IV + max_length for AuthHash\n",
        "    estimated_total_embedding_size_bytes = num_documents * estimated_single_record_size_bytes\n",
        "    estimated_embedding_size_mb = estimated_total_embedding_size_bytes / (1024 * 1024)\n",
        "    print(f\"Estimated encrypted embedding data size for '{dataset_name}': {estimated_embedding_size_mb:.2f} MB (including IV/AuthHash as VARCHAR)\")\n",
        "\n",
        "    ingestion_metrics[dataset_name] = {\n",
        "        \"total_ingestion_time_seconds\": total_ingestion_time,\n",
        "        \"estimated_embedding_size_mb\": estimated_embedding_size_mb,\n",
        "        \"num_documents\": num_documents\n",
        "    }\n",
        "\n",
        "    print(f\"Creating HNSW index for '{collection_name}'...\")\n",
        "    index_params = zilliz_client.prepare_index_params()\n",
        "    # Index on 'encrypted_vector'\n",
        "    index_params.add_index(field_name=\"encrypted_vector\", index_type=\"HNSW\", metric_type=\"COSINE\")\n",
        "    zilliz_client.create_index(collection_name=collection_name, index_params=index_params)\n",
        "    print(f\"HNSW index created for '{collection_name}'.\")\n",
        "\n",
        "    print(f\"Loading collection '{collection_name}' into memory...\")\n",
        "    zilliz_client.load_collection(collection_name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWwrXC0472T-"
      },
      "source": [
        "# Running queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8JdklLCIvOJ"
      },
      "outputs": [],
      "source": [
        "QUERY_BATCH_SIZE = 10 # Process queries in batches for embedding and searching - 10 is the max allowed by Zilliz\n",
        "MAX_SEARCH_RESULTS = 100 # Retrieve top 100 for max k_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF13Va3NMjCx"
      },
      "outputs": [],
      "source": [
        "def get_zilliz_search_results_dcpe(\n",
        "    zilliz_client: MilvusClient,\n",
        "    embedding_model: SentenceTransformer,\n",
        "    collection_name: str,\n",
        "    queries: dict,\n",
        "    dcpe_key: dcpe.DCPEKey, # DCPE key\n",
        "    dcpe_approx_factor: float, # DCPE approximation factor\n",
        "    top_k: int = MAX_SEARCH_RESULTS\n",
        ") -> tuple[dict, list]:\n",
        "    print(f\"Retrieving results from '{collection_name}' for {len(queries)} queries, top_k={top_k}...\")\n",
        "\n",
        "    search_results = {}\n",
        "    query_ids = list(queries.keys())\n",
        "    query_texts = [queries[qid] for qid in query_ids]\n",
        "\n",
        "    all_query_latencies_ms = []\n",
        "\n",
        "    for i in tqdm(range(0, len(query_ids), QUERY_BATCH_SIZE), desc=f\"Searching {collection_name}\"):\n",
        "        batch_query_ids = query_ids[i:i + QUERY_BATCH_SIZE]\n",
        "        batch_query_texts = query_texts[i:i + QUERY_BATCH_SIZE]\n",
        "\n",
        "        start_query_embedding = time.perf_counter()\n",
        "        original_batch_vectors = embedding_model.encode(batch_query_texts, convert_to_list=True)\n",
        "        # MODIFIED: Encrypt query vectors - apply .tolist() to each vector before encryption\n",
        "        encrypted_batch_query_vectors = [\n",
        "            dcpe.encrypt_vector(dcpe_key, vec.tolist(), dcpe_approx_factor).ciphertext\n",
        "            for vec in original_batch_vectors\n",
        "        ]\n",
        "        end_query_embedding = time.perf_counter()\n",
        "        query_embedding_time_ms = (end_query_embedding - start_query_embedding) * 1000\n",
        "\n",
        "        search_params = {\n",
        "            \"metric_type\": \"COSINE\",\n",
        "            \"params\": {\"ef\": max(top_k, 128)}\n",
        "        }\n",
        "\n",
        "        start_search_call = time.perf_counter()\n",
        "        try:\n",
        "            # Search on 'encrypted_vector' field\n",
        "            hits_per_query = zilliz_client.search(\n",
        "                collection_name=collection_name,\n",
        "                data=encrypted_batch_query_vectors, # Use encrypted vectors for search\n",
        "                limit=top_k,\n",
        "                output_fields=[\"doc_id\"],\n",
        "                search_params=search_params\n",
        "            )\n",
        "            end_search_call = time.perf_counter()\n",
        "            search_call_duration_ms = (end_search_call - start_search_call) * 1000\n",
        "\n",
        "            if batch_query_ids:\n",
        "                latency_per_query_in_batch = search_call_duration_ms / len(batch_query_ids)\n",
        "                all_query_latencies_ms.extend([latency_per_query_in_batch] * len(batch_query_ids))\n",
        "\n",
        "            for q_idx, query_id in enumerate(batch_query_ids):\n",
        "                query_hits = {}\n",
        "                for hit in hits_per_query[q_idx]:\n",
        "                    query_hits[str(hit.id)] = float(hit.distance)\n",
        "                search_results[str(query_id)] = query_hits\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during search for batch starting with {batch_query_ids[0]}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Finished retrieving results from '{collection_name}'.\")\n",
        "    return search_results, all_query_latencies_ms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPcbe8Z3NDco"
      },
      "outputs": [],
      "source": [
        "# Define the list of k-values for evaluation\n",
        "k_values = [1, 3, 5, 10, 50, 100]\n",
        "\n",
        "# List to store results for all datasets\n",
        "all_evaluation_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqD_xlTc7_ei"
      },
      "source": [
        "## Evaluating Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q psutil memory_profiler pyRAPL"
      ],
      "metadata": {
        "id": "-mzpRjojvIT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from contextlib import contextmanager\n",
        "import psutil, os, time, tracemalloc\n",
        "\n",
        "@contextmanager\n",
        "def resource_probe(label):\n",
        "    proc = psutil.Process(os.getpid())\n",
        "    cpu_t0 = proc.cpu_times().user + proc.cpu_times().system\n",
        "    mem0 = proc.memory_info().rss\n",
        "    tracemalloc.start()\n",
        "    t0 = time.perf_counter()\n",
        "    yield\n",
        "    elapsed = time.perf_counter() - t0\n",
        "    mem_peak = max(stat.size for stat in tracemalloc.take_snapshot().statistics('filename')) if tracemalloc.is_tracing() else 0\n",
        "    tracemalloc.stop()\n",
        "    cpu_t1 = proc.cpu_times().user + proc.cpu_times().system\n",
        "    mem1 = proc.memory_info().rss\n",
        "    cpu_sec = cpu_t1 - cpu_t0\n",
        "    delta_rss = (mem1 - mem0) / 1e6\n",
        "    peak_mb = mem_peak / 1e6\n",
        "    print(f\"[{label}] wall={elapsed*1e3:.2f} ms | CPU={cpu_sec:.3f}s | ΔRSS={delta_rss:.1f} MB | peak={peak_mb:.1f} MB\")"
      ],
      "metadata": {
        "id": "aOgFldchvPOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5SvYYUHOSMd"
      },
      "outputs": [],
      "source": [
        "# Loop through each dataset\n",
        "for dataset_name in subset_beir_data:\n",
        "    print(f\"\\nStarting evaluation for dataset: {dataset_name}\")\n",
        "\n",
        "    qrels = {\n",
        "        str(query_id): {str(doc_id): int(score) for doc_id, score in doc_scores_dict.items()}\n",
        "        for query_id, doc_scores_dict in subset_beir_data[dataset_name][\"qrels\"].items()\n",
        "    }\n",
        "    queries = subset_beir_data[dataset_name][\"queries\"]\n",
        "\n",
        "    # Skip evaluation if there are no queries or qrels in the subset\n",
        "    if not queries or not qrels:\n",
        "        print(f\"  Skipping evaluation for '{dataset_name}' as the query or qrels subset is empty.\")\n",
        "        continue\n",
        "\n",
        "    collection_name = f\"subset_beir_{dataset_name.replace('-', '_')}_dcpe\"\n",
        "\n",
        "    print(f\"Retrieving search results from Zilliz for '{dataset_name}'...\")\n",
        "\n",
        "    # Wrap query retrieval in resource probe\n",
        "    with resource_probe(f\"{dataset_name}_query_eval\"):\n",
        "        results, query_latencies_ms = get_zilliz_search_results_dcpe(\n",
        "            zilliz_client=zilliz_client,\n",
        "            embedding_model=embedding_model,\n",
        "            collection_name=collection_name,\n",
        "            queries=queries,\n",
        "            dcpe_key=DCPE_KEY,\n",
        "            dcpe_approx_factor=DCPE_APPROXIMATION_FACTOR,\n",
        "            top_k=max(k_values)\n",
        "        )\n",
        "\n",
        "    if not results:\n",
        "        print(f\"No results retrieved for '{dataset_name}'. Skipping evaluation for this dataset.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Retrieved {len(results)} queries' results for '{dataset_name}'.\")\n",
        "\n",
        "    avg_search_latency_ms = 0\n",
        "    p90_search_latency_ms = 0\n",
        "    p99_search_latency_ms = 0\n",
        "\n",
        "    if query_latencies_ms:\n",
        "        avg_search_latency_ms = np.mean(query_latencies_ms)\n",
        "        p90_search_latency_ms = np.percentile(query_latencies_ms, 90)\n",
        "        p99_search_latency_ms = np.percentile(query_latencies_ms, 99)\n",
        "\n",
        "    print(f\"  Avg Search Latency: {avg_search_latency_ms:.2f} ms\")\n",
        "    print(f\"  P90 Search Latency: {p90_search_latency_ms:.2f} ms\")\n",
        "    print(f\"  P99 Search Latency: {p99_search_latency_ms:.2f} ms\")\n",
        "\n",
        "    # Save only operational metrics\n",
        "    dataset_results = {\n",
        "        \"Dataset\": f\"{dataset_name}_subset\",\n",
        "        \"Avg_Search_Latency_ms\": avg_search_latency_ms,\n",
        "        \"P90_Search_Latency_ms\": p90_search_latency_ms,\n",
        "        \"P99_Search_Latency_ms\": p99_search_latency_ms,\n",
        "        \"Total_Ingestion_Time_s\": ingestion_metrics[dataset_name][\"total_ingestion_time_seconds\"],\n",
        "        \"Estimated_Embedding_Size_MB\": ingestion_metrics[dataset_name][\"estimated_embedding_size_mb\"],\n",
        "        \"Num_Documents\": ingestion_metrics[dataset_name][\"num_documents\"]\n",
        "    }\n",
        "    all_evaluation_results.append(dataset_results)\n",
        "\n",
        "    print(f\"Evaluation for '{dataset_name}' complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sEztddG8CRK"
      },
      "source": [
        "# Visualizing the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7evhyxWYqMg"
      },
      "outputs": [],
      "source": [
        "# --- Flatten and build DataFrame (resource metrics only) ---\n",
        "flattened_results = []\n",
        "for dataset_result in all_evaluation_results:\n",
        "    row = {\n",
        "        \"Dataset\": dataset_result[\"Dataset\"],\n",
        "        \"Avg_Search_Latency_ms\": dataset_result[\"Avg_Search_Latency_ms\"],\n",
        "        \"P90_Search_Latency_ms\": dataset_result[\"P90_Search_Latency_ms\"],\n",
        "        \"P99_Search_Latency_ms\": dataset_result[\"P99_Search_Latency_ms\"],\n",
        "        \"Total_Ingestion_Time_s\": dataset_result[\"Total_Ingestion_Time_s\"],\n",
        "        \"Estimated_Embedding_Size_MB\": dataset_result[\"Estimated_Embedding_Size_MB\"],\n",
        "        \"Num_Documents\": dataset_result[\"Num_Documents\"]\n",
        "    }\n",
        "    flattened_results.append(row)\n",
        "\n",
        "results_df = pd.DataFrame(flattened_results)\n",
        "\n",
        "# Define operational columns\n",
        "operational_columns = [\n",
        "    \"Total_Ingestion_Time_s\", \"Estimated_Embedding_Size_MB\",\n",
        "    \"Avg_Search_Latency_ms\", \"P90_Search_Latency_ms\", \"P99_Search_Latency_ms\",\n",
        "    \"Num_Documents\"\n",
        "]\n",
        "ordered_columns = ['Dataset'] + operational_columns\n",
        "results_df = results_df[ordered_columns]\n",
        "\n",
        "print(\"\\n--- Consolidated Resource Utilization Results (DCPE Encrypted) ---\")\n",
        "print(results_df.to_string())\n",
        "\n",
        "# --- Resource Utilization Visualization (Bar Plots) ---\n",
        "operational_plot_data = []\n",
        "for dataset_result in all_evaluation_results:\n",
        "    operational_plot_data.append({\n",
        "        \"Dataset\": dataset_result[\"Dataset\"],\n",
        "        \"Total Ingestion Time (s)\": dataset_result[\"Total_Ingestion_Time_s\"],\n",
        "        \"Estimated Embedding Size (MB)\": dataset_result[\"Estimated_Embedding_Size_MB\"],\n",
        "        \"Avg Search Latency (ms)\": dataset_result[\"Avg_Search_Latency_ms\"],\n",
        "        \"P90 Search Latency (ms)\": dataset_result[\"P90_Search_Latency_ms\"],\n",
        "        \"P99 Search Latency (ms)\": dataset_result[\"P99_Search_Latency_ms\"]\n",
        "    })\n",
        "\n",
        "operational_df = pd.DataFrame(operational_plot_data)\n",
        "\n",
        "if not operational_df.empty:\n",
        "    plt.figure(figsize=(20, 7))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.barplot(x=\"Dataset\", y=\"Total Ingestion Time (s)\", data=operational_df, palette=\"crest\")\n",
        "    plt.title(\"Total Ingestion Time\", fontsize=14)\n",
        "    plt.ylabel(\"Seconds\", fontsize=10)\n",
        "    plt.xlabel(\"\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.barplot(x=\"Dataset\", y=\"Estimated Embedding Size (MB)\", data=operational_df, palette=\"mako\")\n",
        "    plt.title(\"Embedding Size\", fontsize=14)\n",
        "    plt.ylabel(\"MB\", fontsize=10)\n",
        "    plt.xlabel(\"\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    latency_melted_df = operational_df.melt(id_vars=['Dataset'],\n",
        "                                            value_vars=[\"Avg Search Latency (ms)\", \"P90 Search Latency (ms)\", \"P99 Search Latency (ms)\"],\n",
        "                                            var_name=\"Latency Type\", value_name=\"Latency (ms)\")\n",
        "    sns.barplot(x=\"Latency Type\", y=\"Latency (ms)\", hue=\"Dataset\", data=latency_melted_df, palette=\"flare\")\n",
        "    plt.title(\"Query Latency\", fontsize=14)\n",
        "    plt.ylabel(\"ms\", fontsize=10)\n",
        "    plt.xlabel(\"\")\n",
        "    plt.xticks(rotation=20, ha='right')\n",
        "    plt.legend(title=\"Dataset\", loc='best')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No operational metrics data available for plotting.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}