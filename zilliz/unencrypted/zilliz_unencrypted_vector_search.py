# -*- coding: utf-8 -*-
"""zilliz_unencrypted_vector_search.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P6xZ_o7CImcxJDUWP5K6d4DHPoVv2yVw
"""

!pip install -qU beir sentence-transformers pymilvus datasets

!pip install --upgrade --quiet torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118

import os
from beir import util
from beir.datasets.data_loader import GenericDataLoader
from beir.retrieval.evaluation import EvaluateRetrieval
import pandas as pd
from pymilvus import MilvusClient, FieldSchema, DataType, CollectionSchema, Collection
from sentence_transformers import SentenceTransformer
import torch
from google.colab import userdata
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import time
import numpy as np

datasets_to_load = ["nfcorpus", "fiqa", "scidocs"]
beir_data_path = "./beir_datasets" # Local directory to store BEIR data
os.makedirs(beir_data_path, exist_ok=True)
loaded_beir_data = {}

for dataset_name in datasets_to_load:
    print(f"\nProcessing dataset: {dataset_name}")

    # Step 3a: Download the dataset
    url = f"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset_name}.zip"
    out_dir = os.path.join(beir_data_path, dataset_name)

    if not os.path.exists(out_dir):
        print(f"Downloading {dataset_name} from {url} to {out_dir}...")
        data_path = util.download_and_unzip(url, out_dir)
        print(f"Downloaded {dataset_name} to: {data_path}")
    else:
        print(f"Dataset {dataset_name} already exists at {out_dir}. Skipping download.")
        data_path = out_dir

    # Step 3b: Load the corpus, queries, and qrels
    try:
        corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split="test")
        loaded_beir_data[dataset_name] = {
            "corpus": corpus,
            "queries": queries,
            "qrels": qrels
        }
        print(f"Successfully loaded data for {dataset_name}.")
        print(f"  Corpus size: {len(corpus)} documents")
        print(f"  Queries size: {len(queries)} queries")
        print(f"  Qrels size: {len(qrels)} relevance judgments")

        # Print a sample document and query to verify
        if len(corpus) > 0:
            sample_doc_id = list(corpus.keys())[0]
            print(f"  Sample corpus entry ({sample_doc_id}): {corpus[sample_doc_id]['title']} - {corpus[sample_doc_id]['text'][:100]}...")
        if len(queries) > 0:
            sample_query_id = list(queries.keys())[0]
            print(f"  Sample query entry ({sample_query_id}): {queries[sample_query_id][:100]}...")

    except Exception as e:
        print(f"Error loading {dataset_name}: {e}")

ZILLIZ_CLOUD_URI = userdata.get("ZILLIZ_ENDPOINT")
ZILLIZ_CLOUD_API_KEY = userdata.get("ZILLIZ_TOKEN")

zilliz_client = MilvusClient(
        uri=ZILLIZ_CLOUD_URI,
        token=ZILLIZ_CLOUD_API_KEY
    )

device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the model
try:
    embedding_model = SentenceTransformer("all-MiniLM-L6-v2", device=device)
    print(f"Model 'all-MiniLM-L6-v2' loaded successfully on {device}.")
    # Verify model output dimension (all-MiniLM-L6-v2 has 384 dimensions)
    dummy_embedding = embedding_model.encode("test sentence")
    print(f"Model output dimension: {len(dummy_embedding)}")

except Exception as e:
    print(f"Failed to load embedding model: {e}")

DIMENSION = 384 # Dimension for all-MiniLM-L6-v2
BATCH_SIZE = 64
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
ingestion_metrics = {}

for dataset_name in loaded_beir_data:
    print(f"\nProcessing Zilliz ingestion for dataset: {dataset_name}")

    # Zilliz collection names are case-sensitive and prefer underscores
    collection_name = f"beir_{dataset_name.replace('-', '_')}"

    # Define and Create Collection

    # Check if collection already exists to avoid recreation errors
    if zilliz_client.has_collection(collection_name=collection_name):
        print(f"Collection '{collection_name}' already exists. Dropping and recreating.")
        zilliz_client.drop_collection(collection_name=collection_name)

    fields = [
        FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=512, is_primary=True),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535), # Max length for string type
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)
    ]
    schema = CollectionSchema(fields, description=f"BEIR {dataset_name} documents")

    print(f"Creating collection '{collection_name}'...")
    zilliz_client.create_collection(
        collection_name=collection_name,
        schema=schema,
        consistency_level="Strong"
    )
    print(f"Collection '{collection_name}' created successfully.")

    # Iterate, Embed, Prepare, and Insert
    corpus = loaded_beir_data[dataset_name]["corpus"]

    # Prepare data for batching
    doc_ids = list(corpus.keys())
    num_documents = len(doc_ids)

    start_total_ingestion = time.perf_counter()

    documents_to_embed = [] # List of concatenated "title text"
    original_titles = []
    original_texts = []

    for doc_id in doc_ids:
        title = corpus[doc_id].get("title", "")
        text = corpus[doc_id].get("text", "")

        # Concatenate title and text for embedding as recommended for relevance
        content_to_embed = f"{title} {text}".strip()

        documents_to_embed.append(content_to_embed)
        original_titles.append(title)
        original_texts.append(text)

    print(f"Generating embeddings and inserting data for '{dataset_name}' corpus ({len(doc_ids)} documents)...")

    # Process in batches
    for i in tqdm(range(0, len(doc_ids), BATCH_SIZE), desc=f"Ingesting {dataset_name}"):
        batch_doc_ids = doc_ids[i:i + BATCH_SIZE]
        batch_contents = documents_to_embed[i:i + BATCH_SIZE]
        batch_titles = original_titles[i:i + BATCH_SIZE]
        batch_texts = original_texts[i:i + BATCH_SIZE]

        # Generate embeddings for the current batch
        # Ensure conversion to list as per pymilvus insert requirements
        batch_vectors = embedding_model.encode(batch_contents, convert_to_list=True)

        # Prepare entities for insertion
        entities = []
        for j in range(len(batch_doc_ids)):
            entities.append({
                "doc_id": batch_doc_ids[j],
                "title": batch_titles[j],
                "text": batch_texts[j],
                "vector": batch_vectors[j]
            })

        # Insert the batch
        zilliz_client.insert(
            collection_name=collection_name,
            data=entities
        )

    # Calculate ingestion time
    end_total_ingestion = time.perf_counter()
    total_ingestion_time = end_total_ingestion - start_total_ingestion
    print(f"Finished inserting data for '{dataset_name}'. Total ingestion time: {total_ingestion_time:.2f} seconds")

    # Calculate embedding size
    estimated_embedding_size_bytes = num_documents * DIMENSION * 4
    estimated_embedding_size_mb = estimated_embedding_size_bytes / (1024 * 1024) # Convert to MB
    print(f"Estimated raw embedding data size for '{dataset_name}': {estimated_embedding_size_mb:.2f} MB")

    # Store these metrics
    ingestion_metrics[dataset_name] = {
        "total_ingestion_time_seconds": total_ingestion_time,
        "estimated_embedding_size_mb": estimated_embedding_size_mb,
        "num_documents": num_documents
    }

    # Create HNSW Index
    print(f"Creating HNSW index for '{collection_name}'...")
    index_params = zilliz_client.prepare_index_params()
    index_params.add_index(
        field_name="vector",
        index_type="HNSW",
        metric_type="COSINE",
    )
    zilliz_client.create_index(
        collection_name=collection_name,
        index_params=index_params
    )
    print(f"HNSW index created for '{collection_name}'.")

    # Step: Load collection into memory
    print(f"Loading collection '{collection_name}' into memory...")
    zilliz_client.load_collection(collection_name=collection_name)
    print(f"Collection '{collection_name}' loaded successfully.")

QUERY_BATCH_SIZE = 10 # Process queries in batches for embedding and searching - 10 is the max allowed by Zilliz
MAX_SEARCH_RESULTS = 100 # Retrieve top 100 for max k_value

def get_zilliz_search_results(
    zilliz_client: MilvusClient,
    embedding_model: SentenceTransformer,
    collection_name: str,
    queries: dict, # Format: {query_id: query_text, ...}
    top_k: int = MAX_SEARCH_RESULTS
) -> tuple[dict, list]:
    """
    Performs vector similarity search on a Zilliz collection for given queries
    and formats results for BEIR evaluation.

    Args:
        zilliz_client: An initialized MilvusClient instance.
        embedding_model: The loaded SentenceTransformer model.
        collection_name: The name of the Zilliz collection to search.
        queries: A dictionary of queries with query_id as key and query_text as value.
        top_k: The maximum number of results to retrieve per query.

    Returns:
        A dictionary in BEIR evaluation format: {query_id: {doc_id: score, ...}}
    """
    print(f"Retrieving results from '{collection_name}' for {len(queries)} queries, top_k={top_k}...")

    search_results = {}
    query_ids = list(queries.keys())
    query_texts = [queries[qid] for qid in query_ids]

    # List to store individual query latencies
    all_query_latencies_ms = []

    # Process queries in batches for efficient embedding and searching
    for i in tqdm(range(0, len(query_ids), QUERY_BATCH_SIZE), desc=f"Searching {collection_name}"):
        batch_query_ids = query_ids[i:i + QUERY_BATCH_SIZE]
        batch_query_texts = query_texts[i:i + QUERY_BATCH_SIZE]

        # Measure embedding time for queries
        start_query_embedding = time.perf_counter()
        # Generate embeddings for the current batch of queries
        batch_query_vectors = embedding_model.encode(batch_query_texts, convert_to_list=True)
        end_query_embedding = time.perf_counter()
        query_embedding_time_ms = (end_query_embedding - start_query_embedding) * 1000


        # Define search parameters for HNSW with COSINE similarity
        search_params = {
            "metric_type": "COSINE",
            "params": {"ef": max(top_k, 128)}
        }

        # Measure Zilliz search call duration
        start_search_call = time.perf_counter()

        # Perform the batched search in Zilliz
        try:
            hits_per_query = zilliz_client.search(
                collection_name=collection_name,
                data=batch_query_vectors,
                limit=top_k,
                output_fields=["doc_id"], # Request the original doc_id from the collection
                search_params=search_params
            )
            end_search_call = time.perf_counter()
            search_call_duration_ms = (end_search_call - start_search_call) * 1000


            # Append latency for each query in the batch
            if batch_query_ids: # Avoid division by zero
                latency_per_query_in_batch = search_call_duration_ms / len(batch_query_ids)
                all_query_latencies_ms.extend([latency_per_query_in_batch] * len(batch_query_ids))


            # Process hits for each query in the batch
            for q_idx, query_id in enumerate(batch_query_ids):
                query_hits = {}
                # Each hit is a Hit object with 'id' and 'distance' attributes
                for hit in hits_per_query[q_idx]:
                    query_hits[str(hit.id)] = float(hit.distance)
                search_results[str(query_id)] = query_hits # Ensure query_id is string

        except Exception as e:
            print(f"Error during search for batch starting with {batch_query_ids[0]}: {e}")
            continue

    print(f"Finished retrieving results from '{collection_name}'.")
    return search_results, all_query_latencies_ms

# Define the list of k-values for evaluation
k_values = [1, 3, 5, 10, 50, 100]

# List to store results for all datasets
all_evaluation_results = []

# Loop through each dataset
for dataset_name in loaded_beir_data:
    print(f"\nStarting evaluation for dataset: {dataset_name}")

    # Ensure qrels are correctly formatted with string query_ids and doc_ids, and int scores
    qrels = {
        str(qid): {str(did): int(score) for did, score in doc_scores.items()}
        for qid, doc_scores in loaded_beir_data[dataset_name]["qrels"].items()
    }
    queries = loaded_beir_data[dataset_name]["queries"]
    corpus = loaded_beir_data[dataset_name]["corpus"]

    # Construct collection name
    collection_name = f"beir_{dataset_name.replace('-', '_')}"

    # Use the retriever function to get search results
    print(f"Retrieving search results from Zilliz for '{dataset_name}'...")
    results, query_latencies_ms = get_zilliz_search_results(
        zilliz_client=zilliz_client,
        embedding_model=embedding_model,
        collection_name=collection_name,
        queries=queries,
        top_k=max(k_values)
    )

    if not results:
        print(f"No results retrieved for '{dataset_name}'. Skipping evaluation for this dataset.")
        continue

    print(f"Retrieved {len(results)} queries' results for '{dataset_name}'.")

    # Calculate search latency metrics (Avg, P90, P99)
    avg_search_latency_ms = 0
    p90_search_latency_ms = 0
    p99_search_latency_ms = 0

    if query_latencies_ms:
        avg_search_latency_ms = np.mean(query_latencies_ms)
        p90_search_latency_ms = np.percentile(query_latencies_ms, 90)
        p99_search_latency_ms = np.percentile(query_latencies_ms, 99)
    print(f"  Avg Search Latency: {avg_search_latency_ms:.2f} ms")
    print(f"  P90 Search Latency: {p90_search_latency_ms:.2f} ms")
    print(f"  P99 Search Latency: {p99_search_latency_ms:.2f} ms")


    print(f"Calculating evaluation metrics for '{dataset_name}'...")

    retriever_evaluator = EvaluateRetrieval()

    # Unpack the tuple returned by .evaluate()
    ndcg_scores, map_scores, recall_scores, precision_scores = retriever_evaluator.evaluate(qrels, results, k_values)

    dataset_results = {
        "Dataset": dataset_name,
        "NDCG@k": ndcg_scores,
        "MAP@k": map_scores,
        "Recall@k": recall_scores,
        "Precision@k": precision_scores, # This is the dictionary for P@k
        # ADD NEW OPERATIONAL METRICS HERE:
        "Avg_Search_Latency_ms": avg_search_latency_ms,
        "P90_Search_Latency_ms": p90_search_latency_ms,
        "P99_Search_Latency_ms": p99_search_latency_ms,
        "Total_Ingestion_Time_s": ingestion_metrics[dataset_name]["total_ingestion_time_seconds"],
        "Estimated_Embedding_Size_MB": ingestion_metrics[dataset_name]["estimated_embedding_size_mb"],
        "Num_Documents": ingestion_metrics[dataset_name]["num_documents"]
    }
    all_evaluation_results.append(dataset_results)

    print(f"Evaluation for '{dataset_name}' complete.")
    # Print summary for current dataset with corrected key access
    ndcg_10 = dataset_results["NDCG@k"].get('NDCG@10', 'N/A')
    map_10 = dataset_results["MAP@k"].get('MAP@10', 'N/A')
    recall_100 = dataset_results["Recall@k"].get('Recall@100', 'N/A')
    precision_10 = dataset_results["Precision@k"].get('P@10', 'N/A')

    print(f"  NDCG@10: {ndcg_10:.4f}" if isinstance(ndcg_10, (int, float)) else f"  NDCG@10: {ndcg_10}")
    print(f"  MAP@10: {map_10:.4f}" if isinstance(map_10, (int, float)) else f"  MAP@10: {map_10}")
    print(f"  Recall@100: {recall_100:.4f}" if isinstance(recall_100, (int, float)) else f"  Recall@100: {recall_100}")
    print(f"  Precision@10: {precision_10:.4f}" if isinstance(precision_10, (int, float)) else f"  Precision@10: {precision_10}")

flattened_results = []
for dataset_result in all_evaluation_results:
    row = {"Dataset": dataset_result["Dataset"]}

    # Flatten NDCG@k
    for k_value_str, score in dataset_result["NDCG@k"].items():
        row[f"{k_value_str}"] = score

    # Flatten MAP@k
    for k_value_str, score in dataset_result["MAP@k"].items():
        row[f"{k_value_str}"] = score

    # Flatten Recall@k
    for k_value_str, score in dataset_result["Recall@k"].items():
        row[f"{k_value_str}"] = score

    # Flatten Precision@k (keys are P@k, not Precision@k)
    for k_value_str, score in dataset_result["Precision@k"].items():
        row[f"{k_value_str}"] = score

    flattened_results.append(row)

results_df = pd.DataFrame(flattened_results)

# Reorder columns for better readability
# Collect all metric@k column names, then sort them
metric_k_columns = sorted([col for col in results_df.columns if col != 'Dataset'])
ordered_columns = ['Dataset'] + metric_k_columns
results_df = results_df[ordered_columns]

# Print the final DataFrame to the console
print("\n--- Consolidated BEIR Evaluation Results ---")
print(results_df.to_string()) # Use to_string() to prevent truncation for wide tables

beir_plot_data = []
for dataset_result in all_evaluation_results:
    dataset_name = dataset_result["Dataset"]
    for metric_type, scores_dict in [
        ("NDCG", dataset_result["NDCG@k"]),
        ("MAP", dataset_result["MAP@k"]),
        ("Recall", dataset_result["Recall@k"]),
        ("Precision", dataset_result["Precision@k"])
    ]:
        # Iterate through all k_values for each metric
        for k_str, score in scores_dict.items():
            # Extract the integer k value from string like 'NDCG@10' or 'P@10'
            k_val = int(k_str.split('@')[-1])
            beir_plot_data.append({
                "Dataset": dataset_name,
                "Metric Type": metric_type,
                "k": k_val,
                "Score": score
            })
beir_plot_df = pd.DataFrame(beir_plot_data)

if not beir_plot_df.empty:
    # Create a figure with 2 rows and 2 columns for 4 plots
    plt.figure(figsize=(18, 12))
    sns.set_palette("viridis")

    # Plot NDCG
    plt.subplot(2, 2, 1)
    sns.lineplot(x="k", y="Score", hue="Dataset", data=beir_plot_df[beir_plot_df['Metric Type'] == 'NDCG'], marker='o')
    plt.title("NDCG across k-values", fontsize=14)
    plt.xlabel("k", fontsize=10)
    plt.ylabel("NDCG Score", fontsize=10)
    plt.ylim(0, 1.0)
    plt.xticks(k_values) # Ensure all k_values are ticks
    plt.legend(title="Dataset", loc='best')

    # Plot MAP
    plt.subplot(2, 2, 2)
    sns.lineplot(x="k", y="Score", hue="Dataset", data=beir_plot_df[beir_plot_df['Metric Type'] == 'MAP'], marker='o')
    plt.title("MAP across k-values", fontsize=14)
    plt.xlabel("k", fontsize=10)
    plt.ylabel("MAP Score", fontsize=10)
    plt.ylim(0, 1.0)
    plt.xticks(k_values)
    plt.legend(title="Dataset", loc='best')

    # Plot Recall
    plt.subplot(2, 2, 3)
    sns.lineplot(x="k", y="Score", hue="Dataset", data=beir_plot_df[beir_plot_df['Metric Type'] == 'Recall'], marker='o')
    plt.title("Recall across k-values", fontsize=14)
    plt.xlabel("k", fontsize=10)
    plt.ylabel("Recall Score", fontsize=10)
    plt.ylim(0, 1.0)
    plt.xticks(k_values)
    plt.legend(title="Dataset", loc='best')

    # Plot Precision
    plt.subplot(2, 2, 4)
    sns.lineplot(x="k", y="Score", hue="Dataset", data=beir_plot_df[beir_plot_df['Metric Type'] == 'Precision'], marker='o')
    plt.title("Precision across k-values", fontsize=14)
    plt.xlabel("k", fontsize=10)
    plt.ylabel("Precision Score", fontsize=10)
    plt.ylim(0, 1.0)
    plt.xticks(k_values)
    plt.legend(title="Dataset", loc='best')

    plt.tight_layout()
    # plt.savefig("beir_all_metrics_comprehensive.png", dpi=300)
    plt.show()
else:
    print("No BEIR evaluation data available for plotting.")

# --- Operational Metrics Visualization ---
operational_plot_data = []
for dataset_result in all_evaluation_results:
    operational_plot_data.append({
        "Dataset": dataset_result["Dataset"],
        "Total Ingestion Time (s)": dataset_result["Total_Ingestion_Time_s"],
        "Estimated Embedding Size (MB)": dataset_result["Estimated_Embedding_Size_MB"],
        "Avg Search Latency (ms)": dataset_result["Avg_Search_Latency_ms"],
        "P90 Search Latency (ms)": dataset_result["P90_Search_Latency_ms"],
        "P99 Search Latency (ms)": dataset_result["P99_Search_Latency_ms"]
    })
operational_df = pd.DataFrame(operational_plot_data)

if not operational_df.empty:
    plt.figure(figsize=(20, 7)) # Adjust figure size

    # Plot Ingestion Time
    plt.subplot(1, 3, 1)
    sns.barplot(x="Dataset", y="Total Ingestion Time (s)", data=operational_df, palette="crest")
    plt.title("Total Data Ingestion Time", fontsize=14)
    plt.ylabel("Time (seconds)", fontsize=10)
    plt.xlabel("")
    plt.tight_layout() # Ensure labels don't overlap

    # Plot Estimated Embedding Size
    plt.subplot(1, 3, 2)
    sns.barplot(x="Dataset", y="Estimated Embedding Size (MB)", data=operational_df, palette="mako")
    plt.title("Estimated Embedding Storage Size", fontsize=14)
    plt.ylabel("Size (MB)", fontsize=10)
    plt.xlabel("")
    plt.tight_layout()

    # Plot Search Latency (Avg, P90, P99)
    plt.subplot(1, 3, 3)
    latency_melted_df = operational_df.melt(id_vars=['Dataset'],
                                            value_vars=["Avg Search Latency (ms)", "P90 Search Latency (ms)", "P99 Search Latency (ms)"],
                                            var_name="Latency Type", value_name="Latency (ms)")
    sns.barplot(x="Latency Type", y="Latency (ms)", hue="Dataset", data=latency_melted_df, palette="flare")
    plt.title("Vector Search Latency", fontsize=14)
    plt.ylabel("Latency (ms)", fontsize=10)
    plt.xlabel("")
    plt.xticks(rotation=20, ha='right') # Rotate labels for readability
    plt.legend(title="Dataset", loc='best')
    plt.tight_layout()

    # Final layout adjustment for the entire figure
    plt.tight_layout()
    # plt.savefig("operational_metrics_summary.png", dpi=300)
    plt.show()
else:
    print("No operational metrics data available for plotting.")

#  Generate plots or charts to visualize the performance
print("\n--- Generating Visualizations ---")

# Set style for plots
sns.set_style("whitegrid")
plt.figure(figsize=(15, 8))

# Define metrics and k-values to plot
metrics_to_plot = {
    "NDCG": "NDCG@10",
    "MAP": "MAP@10",
    "Recall": "Recall@100",
    "Precision": "P@10" #
}

# Prepare data for plotting (melt the DataFrame)
# Create a long-format DataFrame suitable for Seaborn
plot_data = []
for index, row in results_df.iterrows():
    dataset = row['Dataset']
    for metric_name, col_name in metrics_to_plot.items():
        if col_name in row: # Ensure the column exists
            plot_data.append({
                "Dataset": dataset,
                "Metric": metric_name,
                "Value": row[col_name],
                "K_Value": col_name.split('@')[-1] # Extract K value for labeling
            })

plot_df = pd.DataFrame(plot_data)

if not plot_df.empty:
    # Create a bar plot
    sns.barplot(x="Metric", y="Value", hue="Dataset", data=plot_df, palette="viridis")
    plt.title("Key Retrieval Metrics Across Datasets (at specified k-values)", fontsize=16)
    plt.xlabel("Metric", fontsize=12)
    plt.ylabel("Score", fontsize=12)
    plt.ylim(0, 1.0) # Metrics are usually between 0 and 1
    plt.legend(title="Dataset")
    plt.tight_layout()

    # Save the plot
    # plot_filename = "beir_evaluation_summary.png"
    # plt.savefig(plot_filename, dpi=300)
    # print(f"Plot saved to {plot_filename}")

    plt.show()

